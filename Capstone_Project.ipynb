{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Creating a data warehouse for the immigration data\n",
    "\n",
    "#### Project Summary\n",
    "In this project I have the data in 2 different sources `sas` and `csv` formats.\n",
    "Creating a data warehouse using `postgreSQL` it's capable of doing the job and there is more details about this in the data model\n",
    "section.\n",
    "performing cleaning process on the data befor inserting it on the data warehouse.\n",
    "this data warehouse will be the final product of the project and the main purpos of it will be to answer the analytical queries \n",
    "like one to answer a question like if immigrants flock to states with generally more immigrants\n",
    "i included a query to answer this in the end of the project to make sure that the data warehouse served its purpos\n",
    "The project follows the following steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore the Data\n",
    "* Step 3: Defininge Data Model\n",
    "* Step 4: Runing the ETL\n",
    "* Step 5: Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing all imports \n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import countDistinct, col, split, udf, count, when, isnan\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up spark session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "the data comes in 2 different sources `sas` and `csv` formats.\n",
    "we will know more about the data in the exploring section.\n",
    "the goal of the project is to insert the data in a database resides in postgreSQL.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "we have 3 data sets\n",
    "- I94 Immigration Data: This data comes from the US National Tourism and Trade Office.\n",
    "- U.S. City Demographic Data: This data comes from OpenSoft.\n",
    "- Airport Code Table: This is a simple table of airport codes and corresponding cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the immigration data\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "# reading airports data\n",
    "airport_codes = pd.read_csv('airport-codes_csv.csv', encoding='utf-8')\n",
    "# reading demografhics data \n",
    "us_cities_demographics = pd.read_csv('us-cities-demographics.csv',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore the Data\n",
    "in this section we will explore the data and define the issues on it to cleane them in the `ETL` process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring airports data\n",
    "airport_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_cities_demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244\n"
     ]
    }
   ],
   "source": [
    "#finding the number of countries in the data \n",
    "print(len(airport_codes[\"iso_country\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3096313\n",
      "+---------------------+\n",
      "|count(DISTINCT cicid)|\n",
      "+---------------------+\n",
      "|              3096313|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking if we coulde use the column 'cicid' as a primary key for the dataframe. \n",
    "print(df_spark.count())\n",
    "df2 = df_spark.select(countDistinct(\"cicid\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|  occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender| insnum|airline|admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|    0|    0|     0|     0|     0|      0|      0|    239| 152592| 142457|   802|      0|    0|       1| 1881250|3088187|    238| 138429|3095921| 138429|    802|    477|414269|2982605|  83627|     0|19549|       0|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking for the null values\n",
    "df_spark.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_spark.columns]).show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking the datatypes\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reporting the issues with the data to cleane it in the etl process \n",
    "- about the airport dataset we will fillter by the US country and fix the string issues to insert them to the database.\n",
    "- about the demographics dataset we will groub by the state to be able to connect to the facts table\n",
    "- about the immigration dataset we will fix some missing values and drop some columns most of them missing values and not important then split the dataset to 2 dataframes\n",
    "- the columns names in the demographics dataframe having white spaces and i'll fix this in the loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "the purpos of our porject is to build an analytical house and the best model for that is the `dimensional model`\n",
    "our data model consists of 4 tables and for more details about the tables itselfs check the data dictionary\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "- creating the database and the tables. \n",
    "- load the data into dataframes. \n",
    "- clean the dataframes.\n",
    "- insert the data into the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Createing the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createdb: database creation failed: ERROR:  database \"immigrationdb\" already exists\n"
     ]
    }
   ],
   "source": [
    "#creating the database in postegreSQL\n",
    "!PGPASSWORD=student createdb -h 127.0.0.1 -U student immigrationdb\n",
    "#loading sql magic\n",
    "%load_ext sql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: student@immigrationdb'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connecting to the database\n",
    "%sql postgresql://student:student@127.0.0.1/immigrationdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://student:***@127.0.0.1/immigrationdb\n",
      "(psycopg2.ProgrammingError) relation \"immigrant\" already exists\n",
      " [SQL: 'CREATE TABLE immigrant\\n(\\n  cicid smallint PRIMARY KEY,\\n  i94bir smallint,\\n  i94visa varchar(50),\\n  count decimal(5,2),\\n  dtadfile varchar(50),\\n  visapost varchar(50),\\n  entdepa varchar(50),\\n  entdepd varchar(50),\\n  matflag varchar(50),\\n  biryear smallint,\\n  dtaddto varchar(50),\\n  gender varchar(50),\\n  admnum decimal(5,2),\\n  fltno varchar(50),\\n  visatype varchar(50)\\n);']\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "CREATE TABLE immigrant\n",
    "(\n",
    "  cicid smallint PRIMARY KEY,\n",
    "  i94bir smallint,\n",
    "  i94visa varchar(50),\n",
    "  count decimal(5,2),\n",
    "  dtadfile varchar(50),\n",
    "  visapost varchar(50),\n",
    "  entdepa varchar(50),\n",
    "  entdepd varchar(50),\n",
    "  matflag varchar(50),\n",
    "  biryear smallint,\n",
    "  dtaddto varchar(50),\n",
    "  gender varchar(50),\n",
    "  admnum decimal(5,2),\n",
    "  fltno varchar(50),\n",
    "  visatype varchar(50)\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE demographics\n",
    "(\n",
    "    State_Code                varchar(10) PRIMARY KEY,\n",
    "    Male_Population           INT,\n",
    "    Female_Population         INT,\n",
    "    Total_Population          INT,\n",
    "    Number_of_Veterans        INT,\n",
    "    Foreign_born              INT,\n",
    "    Median_Age                decimal(12,10),\n",
    "    Average_Household_Size    decimal(12,10)\n",
    ");\n",
    "\n",
    "CREATE TABLE airports\n",
    "(\n",
    "    ident            varchar(10) PRIMARY KEY,\n",
    "    type             varchar(50),\n",
    "    elevation_ft    decimal(5,2),\n",
    "    continent        varchar(50),\n",
    "    iso_country      varchar(10),\n",
    "    iso_region       varchar(10),\n",
    "    gps_code         varchar(10),\n",
    "    iata_code        varchar(10),\n",
    "    local_code       varchar(10),\n",
    "    coordinates      varchar,\n",
    "    State            varchar(10)\n",
    ");\n",
    "\n",
    "CREATE TABLE immigration_facts\n",
    "(\n",
    "  cicid smallint PRIMARY KEY,\n",
    "  i94yr smallint,\n",
    "  i94mon smallint,\n",
    "  i94cit smallint,\n",
    "  i94res smallint,\n",
    "  i94port varchar(50),\n",
    "  arrdate varchar(50),\n",
    "  i94addr varchar(50),\n",
    "  depdate varchar(50),\n",
    "  arrival_mode varchar(50)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_data():\n",
    "    \"\"\"\n",
    "    loading the data and returning back dataframes\n",
    "    \"\"\"\n",
    "    df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "    airport_codes = pd.read_csv('airport-codes_csv.csv', encoding='utf-8') \n",
    "    colnames = ['City', 'State', 'Median_Age', 'Male_Population', 'Female_Population', 'Total_Population',\\\n",
    "                'Number_of_Veterans', 'Foreign_born', 'Average_Household_Size', 'State_Code', 'Race', 'Count']\n",
    "    us_cities_demographics = pd.read_csv('us-cities-demographics.csv',sep=';', names=colnames, header=None, skiprows=1)\n",
    "    print('data loaded successfully')\n",
    "    return df_spark, airport_codes, us_cities_demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(df_spark, airport_codes, us_cities_demographics):\n",
    "    \"\"\"\n",
    "    cleaning 3 datafarmes and return back 4 dataframes ready to be inserted into the database \n",
    "    \"\"\"\n",
    "    print('cleaning airports data')\n",
    "    #filtering for US\n",
    "    airport_codes = airport_codes[airport_codes['iso_country'] == 'US' ].copy()\n",
    "    \n",
    "    #creating a new column for the state \n",
    "    airport_codes['State'] = airport_codes['iso_region'].apply(lambda x: x[x.find('-')+1:])\n",
    "    \n",
    "    #droping the columns that have special charactar in thier string \n",
    "    airport_codes.drop(['name','municipality'], axis=1, inplace = True)\n",
    "    \n",
    "    #replacing '-' by '_' form columns so we can insert it in postegreSQL\n",
    "    def replace(x):\n",
    "        p = x.find('-')\n",
    "        x = x[:p] + '_' + x[p+1:]\n",
    "        return x\n",
    "    airport_codes['iso_region'] = airport_codes['iso_region'].apply(replace)\n",
    "    airport_codes['coordinates'] = airport_codes['coordinates'].apply(replace)\n",
    "        \n",
    "    \n",
    "    print('cleaning demographics data')\n",
    "    # preparing demographics data \n",
    "    us_cities_demographics = us_cities_demographics.groupby('State_Code', as_index=False).agg({'Male_Population':'sum',\\\n",
    "                                                                                               'Female_Population':'sum',\\\n",
    "                                                                           'Total_Population':'sum','Number_of_Veterans':'sum',\\\n",
    "                                                                           'Foreign_born':'sum','Median_Age':'mean'\\\n",
    "                                                                           ,'Average_Household_Size':'mean'})\n",
    "    \n",
    "    # casting int for the columns that contain int numbers\n",
    "    us_cities_demographics_columns = ['Male_Population', 'Female_Population',\n",
    "           'Total_Population', 'Number_of_Veterans', 'Foreign_born']\n",
    "    for c in us_cities_demographics_columns:\n",
    "        us_cities_demographics = us_cities_demographics.astype({c:'int'})\n",
    "        \n",
    "    print('cleaning immigration data')\n",
    "    # preparing immigration data\n",
    "    \n",
    "    # selecting the number of rows we need\n",
    "    df_spark = df_spark.limit(1010000)\n",
    "    \n",
    "    # casting int for the columns that contain int numbers\n",
    "    columns = ['cicid','i94yr','i94mon','i94cit','i94res', 'i94bir', 'biryear']\n",
    "    from pyspark.sql.functions import col\n",
    "    for c in columns:\n",
    "        try:\n",
    "            df_spark = df_spark.withColumn(c ,col(c).cast('int'))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # declaring udf funcs\n",
    "    get_date = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "    \n",
    "    @udf(returnType=StringType()) \n",
    "    def mode(x):\n",
    "        try:    \n",
    "            x = int(x)\n",
    "        except:\n",
    "            x = x\n",
    "        if x == 1:\n",
    "            x = 'Air'\n",
    "        elif x == 2:\n",
    "            x = 'Sea'\n",
    "        elif x == 3:\n",
    "            x = 'Land'\n",
    "        else:\n",
    "            x = 'Not reported'\n",
    "        return x\n",
    "    \n",
    "    @udf(returnType=StringType()) \n",
    "    def visa(x):\n",
    "        try:    \n",
    "            x = int(x)\n",
    "        except:\n",
    "            x = x\n",
    "        if x == 1:\n",
    "            x = 'Business'\n",
    "        elif x == 2:\n",
    "            x = 'Pleasure'\n",
    "        elif x == 3:\n",
    "            x = 'Student'\n",
    "        else:\n",
    "            x = 'unknown'\n",
    "        return x\n",
    "    \n",
    "    # applying the udf funcs    \n",
    "    df_spark = df_spark.withColumn(\"i94visa\", visa(df_spark.i94visa))\n",
    "    df_spark = df_spark.withColumn(\"arrival_mode\", mode(df_spark.i94mode))\n",
    "    \n",
    "    # getting the date for 2 columns\n",
    "    df_spark = df_spark.withColumn(\"arrdate\", get_date(df_spark.arrdate))\n",
    "    df_spark = df_spark.withColumn(\"depdate\", get_date(df_spark.depdate))\n",
    "    \n",
    "    #splitting the dataframe into 2 dataframes and converting them to pandas dataframes\n",
    "    \n",
    "    df_facts = df_spark.select(['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'arrdate', 'i94addr', 'depdate', 'arrival_mode'])\n",
    "    df_facts = df_facts.toPandas()\n",
    "\n",
    "    df_immigrant = df_spark.select(['cicid', 'i94yr', 'i94bir', 'i94visa', 'count', 'dtadfile', 'visapost', 'entdepa',\\\n",
    "                                    'entdepd', 'matflag', 'biryear', 'dtaddto', 'gender', 'admnum', 'fltno', 'visatype',])\n",
    "    df_immigrant = df_immigrant.toPandas()\n",
    "    \n",
    "    return airport_codes, us_cities_demographics, df_facts, df_immigrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inserting_data(dataframe, table):\n",
    "    \"\"\"\n",
    "    inserting a dataframe into a spacific table in the databse\n",
    "    \"\"\"\n",
    "    print(f'inserting data into {table} table')\n",
    "    conn_string = 'postgresql://student:student@127.0.0.1/immigrationdb'\n",
    "    db = create_engine(conn_string)\n",
    "    conn = db.connect()\n",
    "    dataframe.to_sql(table , con=conn, if_exists='replace',\n",
    "                      index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded successfully\n",
      "cleaning airports data\n",
      "cleaning demographics data\n",
      "cleaning immigration data\n",
      "inserting data into airports table\n",
      "inserting data into demographics table\n",
      "inserting data into immigration_facts table\n",
      "inserting data into immigrant table\n"
     ]
    }
   ],
   "source": [
    "df_spark, airport_codes, us_cities_demographics = loading_data()\n",
    "airport_codes, us_cities_demographics, df_facts, df_immigrant = cleaning_data(df_spark, airport_codes, us_cities_demographics)\n",
    "inserting_data(airport_codes, 'airports')\n",
    "inserting_data(us_cities_demographics, 'demographics')\n",
    "inserting_data(df_facts, 'immigration_facts')\n",
    "inserting_data(df_immigrant, 'immigrant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "running a quality check the number of the rows to be suer that the data is inserted into the tables\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airports table passes the \"is the data inserted\" quality check\n",
      "demographics table passes the \"is the data inserted\" quality check\n",
      "immigration_facts table passes the \"is the data inserted\" quality check\n",
      "immigrant table passes the \"is the data inserted\" quality check\n",
      "airports table passes the \"constrants\" quality check\n",
      "demographics table passes the \"constrants\" quality check\n",
      "immigration_facts table passes the \"constrants\" quality check\n",
      "immigrant table passes the \"constrants\" quality check\n"
     ]
    }
   ],
   "source": [
    "tables = ['airports', 'demographics', 'immigration_facts', 'immigrant']\n",
    "conn_string = 'postgresql://student:student@127.0.0.1/immigrationdb'\n",
    "conn = psycopg2.connect(conn_string\n",
    "                        )\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "def is_the_data_inserted(tables):\n",
    "    for table in tables:\n",
    "\n",
    "        sql1 = f'''select count(*) from {table} ;'''\n",
    "        cursor.execute(sql1)\n",
    "        for i in cursor.fetchall():\n",
    "            if i[0] > 0:\n",
    "                print(f'{table} table passes the \"is the data inserted?\" quality check')\n",
    "            else :\n",
    "                print(f'{table} table fails the \"is the data inserted?\" quality check')\n",
    "\n",
    "def constrants(table, pk):\n",
    "    \"\"\"\n",
    "    checking if the primary key of the table have null valuse\n",
    "    \"\"\"\n",
    "    sql1 = f'''SELECT count(*) FROM {table} WHERE \"{pk}\" IS NULL ;'''\n",
    "    cursor.execute(sql1)\n",
    "    for i in cursor.fetchall():\n",
    "        if i[0] == 0:\n",
    "            print(f'{table} table passes the \"constrants\" quality check')\n",
    "        else :\n",
    "            print(f'{table} table fails the \"constrants\" quality check')               \n",
    "                \n",
    "is_the_data_inserted(tables)\n",
    "# checking if the primary key of the table have null valuse\n",
    "constrants(\"airports\", \"ident\")\n",
    "constrants(\"demographics\", \"State_Code\")\n",
    "constrants(\"immigration_facts\", \"cicid\")\n",
    "constrants(\"immigrant\", \"cicid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing the logic of the database \n",
    "in the below cell we want to find if the immigrants flock to states with generally more immigrants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://student:***@127.0.0.1/immigrationdb\n",
      "20 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>i94addr</th>\n",
       "        <th>Foreign_born</th>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>CA</td>\n",
       "        <td>37059662</td>\n",
       "        <td>161465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>NY</td>\n",
       "        <td>17186873</td>\n",
       "        <td>186761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>TX</td>\n",
       "        <td>14498054</td>\n",
       "        <td>46414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>FL</td>\n",
       "        <td>7845566</td>\n",
       "        <td>219828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>IL</td>\n",
       "        <td>4632600</td>\n",
       "        <td>28132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>AZ</td>\n",
       "        <td>3411565</td>\n",
       "        <td>7474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>MA</td>\n",
       "        <td>2573815</td>\n",
       "        <td>23609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>NV</td>\n",
       "        <td>2406685</td>\n",
       "        <td>37233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>NJ</td>\n",
       "        <td>2327750</td>\n",
       "        <td>23289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>WA</td>\n",
       "        <td>2204810</td>\n",
       "        <td>14078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>NC</td>\n",
       "        <td>1896635</td>\n",
       "        <td>8406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>CO</td>\n",
       "        <td>1688155</td>\n",
       "        <td>5855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>PA</td>\n",
       "        <td>1439936</td>\n",
       "        <td>9392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>VA</td>\n",
       "        <td>1346270</td>\n",
       "        <td>10329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>MI</td>\n",
       "        <td>1214547</td>\n",
       "        <td>10326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>MD</td>\n",
       "        <td>1148970</td>\n",
       "        <td>8190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>CT</td>\n",
       "        <td>1114250</td>\n",
       "        <td>4880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>MN</td>\n",
       "        <td>1069888</td>\n",
       "        <td>3442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>OR</td>\n",
       "        <td>928765</td>\n",
       "        <td>4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>TN</td>\n",
       "        <td>900149</td>\n",
       "        <td>4525</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('CA', 37059662, 161465),\n",
       " ('NY', 17186873, 186761),\n",
       " ('TX', 14498054, 46414),\n",
       " ('FL', 7845566, 219828),\n",
       " ('IL', 4632600, 28132),\n",
       " ('AZ', 3411565, 7474),\n",
       " ('MA', 2573815, 23609),\n",
       " ('NV', 2406685, 37233),\n",
       " ('NJ', 2327750, 23289),\n",
       " ('WA', 2204810, 14078),\n",
       " ('NC', 1896635, 8406),\n",
       " ('CO', 1688155, 5855),\n",
       " ('PA', 1439936, 9392),\n",
       " ('VA', 1346270, 10329),\n",
       " ('MI', 1214547, 10326),\n",
       " ('MD', 1148970, 8190),\n",
       " ('CT', 1114250, 4880),\n",
       " ('MN', 1069888, 3442),\n",
       " ('OR', 928765, 4375),\n",
       " ('TN', 900149, 4525)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT i94addr, \"Foreign_born\" , count(cicid)\n",
    "FROM immigration_facts i\n",
    "JOIN demographics d\n",
    "ON i.i94addr = d.\"State_Code\"\n",
    "GROUP BY 1 , 2\n",
    "ORDER BY \"Foreign_born\" DESC\n",
    "LIMIT 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we found from the query abouve that there a relation between the new immagrants the state that have big number of immagrants ecept in one state `FL` \n",
    "and here we used our data warehouse to answer a question we couldn't do that by a single dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data dictionary \n",
    "### we have 4 tables\n",
    "#### immigrate table\n",
    "-  cicid    :  uniqe identfire\n",
    "-  i94bir   :  Age of Respondent in Years\n",
    "-  i94visa  :  Visa codes collapsed into three categories: Business, Pleasure, Student\n",
    "-  count    :  Used for summary statistics \n",
    "-  dtadfile :  Character Date Field - Date added to I-94 Files\n",
    "-  visapost :  Department of State where where Visa was issued\n",
    "-  entdepa  :  Arrival Flag - admitted or paroled into the U.S\n",
    "-  entdepd  :  Departure Flag - Departed, lost I-94 or is deceased\n",
    "-  matflag  :  Match flag - Match of arrival and departure records\n",
    "-  biryear  :  4 digit year of birth \n",
    "-  dtaddto  :  Character Date Field - Date to which admitted to U.S. (allowed to stay until)\n",
    "-  gender   :  Non-immigrant sex\n",
    "-  admnum   :  Admission Number\n",
    "-  fltno    :  Flight number of Airline used to arrive in U.S.\n",
    "-  visatype :  Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n",
    "\n",
    "#### demographics table \n",
    "- State_Code                : reprsnt the state\n",
    "- Male_Population           : number of men\n",
    "- Female_Population         : number of females\n",
    "- Total_Population          : total number\n",
    "- Number_of_Veterans        : vereranes number \n",
    "- Foreign_born              : number of foreign born\n",
    "- Median_Age                : the median age\n",
    "- Average_Household_Size    : the mean of the size of the household\n",
    "\n",
    "#### airports table \n",
    "-    ident            : identefire\n",
    "-    type             : airport type\n",
    "-    elevation_ft     : elevation in foot\n",
    "-    continent        : \n",
    "-    iso_country      : country\n",
    "-    iso_region       : region(country + state)\n",
    "-    gps_code         : gps code\n",
    "-    iata_code        : \n",
    "-    local_code       : \n",
    "-    coordinates      : coordinates\n",
    "-    State            : State\n",
    "\n",
    "#### immigration_facts table \n",
    "\n",
    "-  cicid :uniqe identfire\n",
    "-  i94yr : 4 digit year \n",
    "-  i94mon :Numeric month\n",
    "-  i94cit : city code\n",
    "-  i94res : resedent city code\n",
    "-  i94port : port code\n",
    "-  arrdate : the Arrival Date in the USA.\n",
    "-  i94addr : state code\n",
    "-  depdate : the Departure Date from the USA.\n",
    "-  arrival_mode : 'Air' 'Sea' 'Land' 'Not reported'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in this project i used `pandas` to handle the small datasets and `spark` for the big ones and there is a reason to choose the 2 techs, in gerneral i use pandas dataframe to insert the data into postgresql and with the small datasets using pandas is a good choice and no need for spark cause the process is fast, but in the immigration dataset with this larg number of rows it's hard to process the data with pandas and if we run spark in distributed system it will be 100x faster.\n",
    "- using `postgreSQL` for the database case it can handle about 9.7 millions rows and our data is much smaller, in general we need a releational database fo our dimintional model and according to the size of our data it's not nessessary to go to a cloud data warehouse, to avoid the unnessesary cost we could use `postgreSQL` and also `postgreSQL` has a much better query optimizer, vastly better join handling, and much more flexibility in querying in general than MySQL, which is a big advantage in an analytics environment such as a data warehouse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the sorce of the immigration data is partisiond monthlly so that shoud be the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if The data was increased by 100x then we should us a map reduce tech to process the data in multiple nodes and I go for `redshift`\n",
    "- if The data populates a dashboard that must be updated on a daily basis by 7am every day we should use air flow to schedule the task as we need\n",
    "- if The database needed to be accessed by 100+ people movig to `redshift` can handle this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
